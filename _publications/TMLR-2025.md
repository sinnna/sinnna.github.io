---
title: "Perturbed gradient descent via convex quadratic approximation for nonconvex bilevel optimization"
collection: publications
category: preprints
permalink: /publication/TMLR-2025
excerpt: 'We develop an efficient gradient-based method to decrease the upper-level objective, coupled with a convex Quadratic Program (QP) that minimally perturbed the gradient descent directions to reduce the sub-optimality of the condition imposed by the lower-level problem.'
date: 2025/01
venue: 'Under Review'
# slidesurl: 'https://academicpages.github.io/files/slides1.pdf'
paperurl: 'https://arxiv.org/pdf/2504.17215'
# bibtexurl: 'https://academicpages.github.io/files/bibtex1.bib'
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---
Bilevel optimization is a fundamental tool in hierarchical decision-making and has been
widely applied to machine learning tasks such as hyperparameter tuning, meta-learning, and
continual learning. While significant progress has been made in bilevel optimization, exist-
ing methods predominantly focus on the nonconvex-strongly convex, or the nonconvex-PL
settings, leaving the more general nonconvex-nonconvex framework underexplored. In this
paper, we address this gap by developing an efficient gradient-based method to decrease the
upper-level objective, coupled with a convex Quadratic Program (QP) that minimally per-
turbed the gradient descent directions to reduce the sub-optimality of the condition imposed
by the lower-level problem. We provide a rigorous convergence analysis, demonstrating that
under the existence of a KKT point and a regularity assumption (lower-level norm-squared
gradient satisfies PL), our method achieves an iteration complexity of O(1/ϵ1.5) in terms of
the squared norm of the KKT residual for the reformulated problem. Moreover, even in the
absence of the regularity assumption, we establish an iteration complexity of O(1/ϵ3) for the
same metric. Through extensive numerical experiments on convex and nonconvex synthetic
benchmarks and a data hyper-cleaning task, we illustrate the efficiency and scalability of our
approach.
