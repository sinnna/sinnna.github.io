---
title: "Gradient-regularized out-of-distribution detection"
collection: publications
category: manuscripts
permalink: /publication/ECCV-2024
excerpt: ''
authors: '<b>S. Sharifi</b>, T. Entesari, B. Safaei, V. Patel, M. Fazlyab'
date: 2024/01
venue: '2024 European Conference on Computer Vision (ECCV)'
slidesurl: 'https://www.youtube.com/watch?v=FzJqf7ZkrYQ&t=4s'
paperurl: 'https://arxiv.org/pdf/2404.12368?'
# bibtexurl: 'https://academicpages.github.io/files/bibtex1.bib'
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---
One of the challenges for neural networks in real-life applications is the overconfident errors these models make when the data
is not from the original training distribution. Addressing this issue is
known as Out-of-Distribution (OOD) detection. Many state-of-the-art
OOD methods employ an auxiliary dataset as a surrogate for OOD data
during training to achieve improved performance. However, these methods fail to fully exploit the local information embedded in the auxiliary
dataset. In this work, we propose the idea of leveraging the information embedded in the gradient of the loss function during training to
enable the network to not only learn a desired OOD score for each sample but also to exhibit similar behavior in a local neighborhood around
each sample. We also develop a novel energy-based sampling method to
allow the network to be exposed to more informative OOD samples during the training phase. This is especially important when the auxiliary
dataset is large. We demonstrate the effectiveness of our method through
extensive experiments on several OOD benchmarks, improving the existing state-of-the-art FPR95 by 4% on our ImageNet experiment. We
further provide a theoretical analysis through the lens of certified robustness and Lipschitz analysis to showcase the theoretical foundation of our
work. 
